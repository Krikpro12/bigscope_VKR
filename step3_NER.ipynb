{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e9e60-4e21-4508-9641-36a466d49114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Загрузка научной модели SciSpacy\n",
    "# nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import tqdm\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70046d89-6abd-4e8f-870e-b8badadd1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bcd2f-cd3e-4186-a4ed-6051c6e7b06a",
   "metadata": {},
   "source": [
    "# Создание размеченного датасета для дообучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92878710-ac86-4c16-b549-a8940c3b19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"TECHNOLOGY\": [\n",
    "        'artificial intelligence', 'big data', 'internet of things (IoT)', \n",
    "        'cloud computing', 'quantum computing', 'edge computing', \n",
    "        'blockchain', '5g networks', 'augmented reality', 'virtual reality', \n",
    "        'serverless computing', 'digital twins', 'microservices', \n",
    "        'event-driven architecture', 'in-memory computing', 'fog computing', \n",
    "        'cyber-physical systems', 'software-defined networks', \n",
    "        'data lakehouse', 'data mesh', 'robotics process automation (RPA)',\n",
    "        'human-computer interaction (HCI)', 'cybersecurity', 'distributed computing', \n",
    "        'neuro-symbolic AI', 'federated learning', 'privacy-preserving AI',\n",
    "        'explainable AI (XAI)', 'multi-cloud environments', 'data observability',\n",
    "        'green computing', 'energy-efficient AI', 'smart contracts',\n",
    "        'autonomous systems', 'immersive technologies', 'zero-trust architecture',\n",
    "        'context-aware computing', 'haptic technologies', 'nanotechnologies'\n",
    "    ],\n",
    "    \"METHOD\": [\n",
    "        'principal component analysis (PCA)', 'bayesian methods', \n",
    "        'markov chains', 'gradient boosting', 'stochastic processes', \n",
    "        'time series decomposition', 'dynamic programming', \n",
    "        'feature engineering', 'dimensionality reduction', \n",
    "        'data sampling', 'data imputation', 'data augmentation', \n",
    "        'semi-supervised learning', 'self-supervised learning', \n",
    "        'meta-learning', 'few-shot learning', 'multi-task learning', \n",
    "        'transfer learning', 'ensemble learning', \n",
    "        'autoregressive integrated moving average (ARIMA)', \n",
    "        'exponential smoothing', 'kalman filtering', 'survival analysis', \n",
    "        'probabilistic graphical models', 'causal inference', \n",
    "        'topic modeling', 'outlier detection', 'active learning', \n",
    "        'neural architecture search', 'attention mechanisms', \n",
    "        'hierarchical clustering', 'contrastive learning',\n",
    "        'adaptive boosting', 'bagging', 'maximum likelihood estimation',\n",
    "        'expectation-maximization', 'manifold learning', 'spectral embedding',\n",
    "        'regularization techniques', 'robust optimization', \n",
    "        'decision analysis', 'swarm intelligence'\n",
    "    ],\n",
    "    \"ALGORITHM\": [\n",
    "        'k-means', 'decision trees', 'random forest', 'svm', \n",
    "        'naive bayes', 'knn', 'logistic regression', 'linear regression', \n",
    "        'genetic algorithms', 'gradient descent', 'simulated annealing', \n",
    "        'particle swarm optimization', 'hill climbing', 'pagerank', \n",
    "        'dijkstra algorithm', 'kruskal algorithm', 'prim algorithm', \n",
    "        'bipartite matching', 'shortest path algorithms', \n",
    "        'hierarchical clustering', 'dbscan', 'mean-shift clustering', \n",
    "        'spectral clustering', 'xgboost', 'lightgbm', 'catboost', \n",
    "        'deep q-learning', 'ppo (proximal policy optimization)', \n",
    "        'transformer models', 'variational inference', \n",
    "        'reinforcement learning algorithms', 'apriori', 'fp-growth', \n",
    "        'monte carlo tree search', 'long short-term memory networks (LSTM)',\n",
    "        'convolutional neural networks (CNN)', 'self-organizing maps', \n",
    "        'word2vec', 'doc2vec', 'collaborative filtering algorithms', \n",
    "        'temporal difference learning', 'value iteration', 'policy gradient methods',\n",
    "        'trust region policy optimization (TRPO)', 'asynchronous advantage actor-critic (A3C)'\n",
    "    ],\n",
    "    \"TASK\": [\n",
    "        'classification', 'clustering', 'regression', \n",
    "        'dimensionality reduction', 'time series forecasting', \n",
    "        'anomaly detection', 'sentiment analysis', 'recommendation systems', \n",
    "        'topic modeling', 'translation', 'object detection', \n",
    "        'speech recognition', 'image segmentation', 'text summarization', \n",
    "        'document classification', 'entity recognition', \n",
    "        'causal modeling', 'explainable AI', 'automated feature extraction',\n",
    "        'intelligent tutoring', 'robot path planning', 'game AI design', \n",
    "        'event detection', 'sequence labeling', 'action recognition',\n",
    "        'domain adaptation', 'knowledge graph construction'\n",
    "    ],\n",
    "    \"MODEL\": [\n",
    "        'linear regression', 'logistic regression', 'decision trees', \n",
    "        'random forest', 'k-nearest neighbors (kNN)', 'support vector machine (SVM)', \n",
    "        'naive bayes', 'multilayer perceptron (MLP)', 'convolutional neural networks (CNN)', \n",
    "        'recurrent neural networks (RNN)', 'transformers', 'bert', \n",
    "        'gpt', 'longformer', 'albert', 'graph neural networks (GNN)', \n",
    "        'autoencoders', 'generative adversarial networks (GAN)', \n",
    "        'tabular neural networks', 'variational autoencoders (VAE)', \n",
    "        'language models', 'vision transformers (ViT)', 'ensemble models', \n",
    "        'mixture of experts', 'meta-modeling', 'neuro-symbolic models',\n",
    "        'zero-shot models', 'contrastive learning models', 'sequence-to-sequence models',\n",
    "        'multi-modal transformers', 'attention-based models', 'biLSTMs', 'capsule networks'\n",
    "    ],\n",
    "    \"TOOL\": [\n",
    "        'apache hadoop', 'apache spark', 'tensorflow', 'pytorch', \n",
    "        'scikit-learn', 'pandas', 'numpy', 'matplotlib', \n",
    "        'seaborn', 'plotly', 'd3.js', 'tableau', 'power bi', \n",
    "        'kafka', 'apache beam', 'apache flink', 'kubeflow', \n",
    "        'mlflow', 'grafana', 'superset', 'aws glue', 'databricks', \n",
    "        'nltk', 'spacy', 'gensim', 'flask', 'streamlit', \n",
    "        'fastapi', 'transformers library', 'huggingface datasets', \n",
    "        'langchain', 'ray tune', 'azure ml', 'google colab', \n",
    "        'anaconda', 'mlpack', 'jupyter lab', 'knime',\n",
    "        'snowflake', 'airflow', 'datadog', 'matplotlib', 'dash', \n",
    "        'highcharts', 'supervised library', 'mlxtend'\n",
    "    ],\n",
    "    \"FRAMEWORK\": [\n",
    "        'tensorflow', 'pytorch', 'keras', 'apache flink', \n",
    "        'apache storm', 'ray', 'dask', 'spark mllib', \n",
    "        'scikit-learn pipelines', 'fastai', 'hugging face transformers', \n",
    "        'langchain', 'ray tune', 'azure ml', 'google ai platform', \n",
    "        'kubeflow pipelines', 'mlflow tracking', 'microsoft synapse', \n",
    "        'amazon sagemaker', 'openvino', 'mlxtend', 'bigdl', \n",
    "        'deeplearning4j', 'onnx runtime', 'paddlepaddle'\n",
    "    ],\n",
    "    \"DATA\": [\n",
    "        'structured data', 'unstructured data', 'semi-structured data', \n",
    "        'time series data', 'historical data', 'geospatial data', \n",
    "        'streaming data', 'synthetic data', 'metadata', \n",
    "        'bigquery datasets', 'delta lake datasets', 'data cubes', \n",
    "        'training data', 'test data', 'validation data', \n",
    "        'annotated datasets', 'graph data', 'relational data', \n",
    "        'vector embeddings', 'text corpora', 'speech data', \n",
    "        'event streams', 'tabular data', 'spatial-temporal datasets',\n",
    "        'encrypted datasets', 'multi-view data'\n",
    "    ],\n",
    "    \"PARAMETER\": [\n",
    "        'learning rate', 'batch size', 'number of layers', \n",
    "        'dropout rate', 'number of epochs', 'activation functions', \n",
    "        'optimizer settings', 'momentum', 'weight decay', \n",
    "        'regularization parameters', 'embedding size', \n",
    "        'sequence length', 'window size', 'attention heads', \n",
    "        'hidden layer size', 'decay rate', 'max iterations', \n",
    "        'gradient clipping', 'loss function', 'hyperparameter ranges', \n",
    "        'convergence thresholds'\n",
    "    ],\n",
    "    \"METRIC\": [\n",
    "        'accuracy', 'precision', 'recall', 'f1-score', \n",
    "        'mean squared error (MSE)', 'root mean squared error (RMSE)', \n",
    "        'mean absolute error (MAE)', 'roc-auc score', 'log loss', \n",
    "        'silhouette score', 'calinski-harabasz index', 'perplexity', \n",
    "        'bleu score', 'rouge score', 'edit distance', \n",
    "        'cosine similarity', 'mean average precision (MAP)', \n",
    "        'dcg', 'ndcg', 'kendall rank correlation', 'pairwise precision', \n",
    "        'normalized mutual information (NMI)', 'jaccard index'\n",
    "    ],\n",
    "    \"APPLICATION\": [\n",
    "        'predictive analytics', 'healthcare analytics', 'genomics', \n",
    "        'fraud detection', 'marketing analytics', 'recommender systems', \n",
    "        'financial analytics', 'cybersecurity', 'traffic prediction', \n",
    "        'robotics', 'autonomous vehicles', 'smart cities', \n",
    "        'natural language processing', 'time series forecasting', \n",
    "        'climate modeling', 'bioinformatics', 'customer lifetime value', \n",
    "        'gaming analytics', 'personalization systems', 'geospatial analysis', \n",
    "        'speech synthesis', 'virtual assistants', 'real-time analytics', \n",
    "        'demand forecasting', 'inventory optimization', \n",
    "        'e-commerce analytics', 'social media analytics', \n",
    "        'precision agriculture', 'renewable energy optimization', \n",
    "        'disaster management', 'financial fraud detection', 'edge AI applications'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89747b31-6c6e-4f0b-9210-904f4b6528da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  шаблоны предложений\n",
    "templates = [\n",
    "    \"The study demonstrates how {} and {} can improve the efficiency of {} tasks.\",\n",
    "    \"Using {}, the authors were able to achieve significant improvements in {} with metrics like {}.\",\n",
    "    \"{} and {} are critical for {} and have been explored in depth in this research.\",\n",
    "    \"This paper compares {} and {} for solving problems related to {}.\",\n",
    "    \"Advanced techniques such as {} and {} have been utilized in applications like {}.\",\n",
    "    \"The experimental setup involved using {} for tasks like {}, achieving notable {}.\",\n",
    "    \"Recent advancements in {} have opened new possibilities for improving {}.\",\n",
    "    \"{} has been combined with {} to create innovative solutions for {}.\",\n",
    "    \"In the context of {}, methods such as {} and {} were extensively evaluated.\",\n",
    "    \"Key contributions include the integration of {} with {} for enhanced {}.\",\n",
    "    \"The authors present a novel approach combining {} and {} to tackle challenges in {}.\",\n",
    "    \"Performance improvements were observed when {} was applied alongside {} for {}.\",\n",
    "    \"The methodology employs {} and {} for robust {} pipelines.\",\n",
    "    \"This approach highlights the synergy between {} and {} in enhancing {} outcomes.\",\n",
    "    \"Case studies demonstrate the practical applications of {} and {} in {} scenarios.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366e055-9449-4e5e-a3a5-bc8de3814480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Функции для нормализации и поиска терминов\n",
    "def normalize_term(term):\n",
    "    \"\"\"Нормализует термин для учета регистра и различных вариаций.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', term.strip().lower())\n",
    "\n",
    "def term_variations(term):\n",
    "    \"\"\"\n",
    "    Генерирует различные вариации термина.\n",
    "    Например: \"Principal Component Analysis\" -> [\"PCA\", \"principal component analysis\"]\n",
    "    \"\"\"\n",
    "    variations = [normalize_term(term)]\n",
    "    # Добавляем аббревиатуры, если они указаны в скобках\n",
    "    if '(' in term and ')' in term:\n",
    "        match = re.search(r'\\((.*?)\\)', term)\n",
    "        if match:\n",
    "            abbr = match.group(1).lower()\n",
    "            full_form = re.sub(r'\\(.*?\\)', '', term).strip().lower()\n",
    "            variations.extend([abbr, full_form])\n",
    "    return variations\n",
    "\n",
    "# Обработка категорий с учетом вариаций\n",
    "def process_categories(categories):\n",
    "    processed = {}\n",
    "    for category, terms in categories.items():\n",
    "        expanded_terms = []\n",
    "        for term in terms:\n",
    "            expanded_terms.extend(term_variations(term))\n",
    "        processed[category] = expanded_terms\n",
    "    return processed\n",
    "\n",
    "# Улучшенная функция для получения категории термина\n",
    "def get_category(tokenized_term, categories):\n",
    "    \"\"\"\n",
    "    Определяет категорию для токенизированного термина.\n",
    "    \"\"\"\n",
    "    for category, terms in categories.items():\n",
    "        for term in terms:\n",
    "            term_tokens = term.split()\n",
    "            if tokenized_term == term_tokens:\n",
    "                return category\n",
    "    return \"O\"\n",
    "\n",
    "# Улучшенная аннотация текста\n",
    "def annotate_text(text, terms, categories):\n",
    "    tokens = text.split()\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    term_dict = {tuple(term.split()): category for category, term_list in categories.items() for term in term_list}\n",
    "\n",
    "    # Ищем термины в тексте\n",
    "    for i in range(len(tokens)):\n",
    "        for length in range(1, min(4, len(tokens) - i + 1)):  # Проверяем термины длиной до 3 токенов\n",
    "            span = tuple(normalize_term(' '.join(tokens[i:i + length])).split())\n",
    "            if span in term_dict:\n",
    "                category = term_dict[span]\n",
    "                labels[i] = f\"B-{category}\"\n",
    "                for j in range(1, length):\n",
    "                    labels[i + j] = f\"I-{category}\"\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "# Функция генерации случайного предложения\n",
    "def generate_sentence(terms):\n",
    "    # Убедимся, что количество выбираемых терминов не превышает длину списка\n",
    "    max_terms = min(len(terms), 5)  # Максимум 5 терминов\n",
    "    selected_terms = random.sample(terms, random.randint(3, max_terms))\n",
    "    template = random.choice(templates)\n",
    "    return template.format(*selected_terms)\n",
    "\n",
    "# Генерация синтетического датасета\n",
    "def generate_synthetic_dataset(categories, num_sentences=10000):\n",
    "    processed_categories = process_categories(categories)\n",
    "    terms = [term for terms_list in processed_categories.values() for term in terms_list]\n",
    "    sentences = []\n",
    "    print(\"Generating synthetic dataset with term variations...\")\n",
    "    for _ in tqdm(range(num_sentences)):\n",
    "        # Проверяем, есть ли хотя бы 3 термина в списке\n",
    "        if len(terms) < 3:\n",
    "            raise ValueError(\"Insufficient terms in the list to generate sentences.\")\n",
    "        sentence = generate_sentence(terms)\n",
    "        annotated = annotate_text(sentence, terms, processed_categories)\n",
    "        sentences.append(annotated)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Генерация датасета\n",
    "synthetic_dataset = generate_synthetic_dataset(categories, num_sentences=10000)\n",
    "\n",
    "# Сохранение в файл\n",
    "with open(\"synthetic_ner_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthetic_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66016f50-e8a1-4540-9b84-b0923d193474",
   "metadata": {},
   "source": [
    "# Процесс дообучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7914bc-18bd-4cce-8c0c-acf95c6304a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ekate\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Train size: 8000, Test size: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de2791b97e458a801052de8ef731e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade33b77a36c4d108cad382ca42eeb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Шаг 1. Загрузка синтетического размеченного датасета\n",
    "with open(\"synthetic_ner_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    synthetic_data = json.load(f)\n",
    "\n",
    "# Преобразование в формат Hugging Face\n",
    "def convert_to_hf_format(data):\n",
    "    tokens = [entry[\"tokens\"] for entry in data]\n",
    "    ner_tags = [entry[\"ner_tags\"] for entry in data]\n",
    "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
    "\n",
    "hf_data = convert_to_hf_format(synthetic_data)\n",
    "\n",
    "# Создание Dataset\n",
    "dataset = Dataset.from_dict(hf_data)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Шаг 2. Токенизация\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Уникальные тэги\n",
    "unique_labels = list(set(tag for tags in hf_data[\"ner_tags\"] for tag in tags))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Токенизация и выравнивание меток\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",  # Добавляем автоматическое выравнивание\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Связь токенов и слов\n",
    "        aligned_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)  # Игнорируем специальные токены ([CLS], [SEP])\n",
    "            else:\n",
    "                aligned_labels.append(label2id[label[word_id]])\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Шаг 3. Загрузка модели\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Шаг 4. Метрики оценки\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = predictions.argmax(-1)\n",
    "    true_labels = [\n",
    "        [id2label[label] for label in label_seq if label != -100]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred_seq, label_seq) if l != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Шаг 5. DataCollator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Используем смешанную точность\n",
    ")\n",
    "\n",
    "# Шаг 7. Обучение с использованием Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf33b341-3e93-4808-a0ff-6f5b2bc27ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 5:34:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.1377083191871643, metrics={'train_runtime': 20081.8453, 'train_samples_per_second': 0.398, 'train_steps_per_second': 0.05, 'total_flos': 2090770931712000.0, 'train_loss': 0.1377083191871643, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6c856a-2689-4f82-b9bb-e45ff31aec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Шаг 8. Сохранение модели\n",
    "trainer.save_model(\"./trained_scibert_ner_model\")\n",
    "tokenizer.save_pretrained(\"./trained_scibert_ner_model\")\n",
    "\n",
    "print(\"Model training completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e340d2-a25c-4abf-8181-b65b1c956173",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc69c3f1-dcd2-479e-a769-db069abf0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Путь к сохранённой модели\n",
    "model_path = \"./trained_scibert_ner_model\"\n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e69b75-923b-4233-946d-5061de7e3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: had, Label: DATA, Score: 0.5473\n",
      "Entity: ##oop, Label: DATA, Score: 0.5225\n",
      "Entity: apache spark, Label: TOOL, Score: 0.9924\n",
      "Entity: distributed computing, Label: TECHNOLOGY, Score: 0.9979\n",
      "Entity: principal component analysis, Label: METHOD, Score: 0.9988\n",
      "Entity: gradient boosting, Label: METHOD, Score: 0.9987\n",
      "Entity: data, Label: APPLICATION, Score: 0.6238\n",
      "Entity: processing, Label: METHOD, Score: 0.3217\n"
     ]
    }
   ],
   "source": [
    "# Создание NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "# Пример текста для тестирования\n",
    "\n",
    "\n",
    "# Разметка текста\n",
    "ner_results = ner_pipeline(example_text)\n",
    "\n",
    "# Печать результатов\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb6849-e36d-497f-9df2-563df4b99f08",
   "metadata": {},
   "source": [
    "# NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abe70f-351c-43b3-8baa-4ca54dae0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "collection = client['VKR1']['proccessed_for_topic_modelling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5ca2cbf-fee1-4222-93c4-bb8ab8bd4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список доменных терминов (можно расширить)\n",
    "domain_terms = [\n",
    "    # Big Data Technologies and Platforms\n",
    "    'hadoop', 'spark', 'flink', 'storm', 'kafka', 'hive', 'pig', 'cassandra',\n",
    "    'hbase', 'mongodb', 'redis', 'elasticsearch', 'solr', 'apache beam',\n",
    "    'apache samza', 'apache apex', 'presto', 'apache drill', 'apache tez',\n",
    "    'kudu', 'druid', 'nifi', 'zookeeper', 'oozie', 'mahout', 'sqoop', 'ambari',\n",
    "    'zeppelin', 'mesos', 'alluxio', 'yarn', 'mapreduce', 'azure hdinsight',\n",
    "    'amazon emr', 'google cloud dataproc', 'cloudera', 'hortonworks', 'databricks',\n",
    "    'clickhouse', 'apache ignite', 'azure synapse', 'dataproc', 'data fusion',\n",
    "    'bigquery', 'snowflake', 'redshift', 'minio', 'delta lake', 'lakehouse',\n",
    "    'trino', 'apache kylin', 'apache carbondata', 'hudi', 'iceberg', 'dremio',\n",
    "    'bigtable', 'open tsdb', 'prometheus', 'grafana loki', 'vectorized io',\n",
    "    'openwhisk', 'kubeflow', 'mlflow', 'apache pulsar', 'apache superset',\n",
    "    'apache ranger', 'datadog', 'new relic', 'dynatrace', 'splunk', 'logstash',\n",
    "    'fluentd', 'apache arrow', 'delta sharing', 'microsoft fabric', 'aws glue',\n",
    "    'athena', 'databricks unity catalog', 'citus', 'greenplum', 'hypertable',\n",
    "    'apache pinot', 'apache heron', 'hazelcast', 'couchbase','ai',\n",
    "\n",
    "    # Programming Languages and Tools\n",
    "    'python', 'scala', 'java', 'r', 'sql', 'nosql', 'julia', 'matlab', 'sas',\n",
    "    'perl', 'go', 'c++', 'rust', 'haskell', 'pig latin', 'hiveql', 'bash',\n",
    "    'shell scripting', 'php', 'typescript', 'javascript', 'groovy', 'terraform',\n",
    "    'ansible', 'ruby', 'fortran', 'erlang', 'f#', 'kotlin', 'elixir', 'nim',\n",
    "    'clojure', 'prolog', 'scheme', 'smalltalk', 'powershell', 'visual basic',\n",
    "    'tcl', 'awk', 'sed', 'dart', 'cobol', 'vbscript', 'actionscript', 'apl',\n",
    "    'postgresql', 'mariadb', 'google bigquery sql', 'pl/pgsql', 'solidity',\n",
    "\n",
    "    # Machine Learning and AI\n",
    "    'machine learning', 'deep learning', 'clustering', 'classification',\n",
    "    'regression', 'association rules', 'k-means', 'random forest',\n",
    "    'gradient boosting', 'svm', 'neural networks', 'decision trees',\n",
    "    'bayesian methods', 'markov chains', 'genetic algorithms', 'reinforcement learning',\n",
    "    'exponential smoothing', 'pca', 't-sne', 'lda', 'linear regression',\n",
    "    'logistic regression', 'naive bayes', 'knn', 'deep neural networks', 'rnn',\n",
    "    'cnn', 'autoencoders', 'transformers', 'attention mechanism', 'seq2seq models',\n",
    "    'ensemble learning', 'bagging', 'boosting', 'xgboost', 'lightgbm', 'catboost',\n",
    "    'hyperparameter tuning', 'grid search', 'bayesian optimization',\n",
    "    'self-supervised learning', 'contrastive learning', 'meta-learning',\n",
    "    'few-shot learning', 'multi-task learning', 'generative adversarial networks',\n",
    "    'variational autoencoders', 'graph neural networks', 'language models',\n",
    "    'bpe tokenization', 'word2vec', 'fasttext', 'doc2vec', 'sentence transformers',\n",
    "    'roberta', 'gpt', 'scibert', 'longformer', 'xlm', 'electra', 'albert',\n",
    "    'vision transformers (vit)', 'bertology', 'zero-shot learning',\n",
    "    'one-shot learning', 'distillation', 'federated gans', 'tabnet', 'deepar',\n",
    "    'fastai', 'hugging face transformers', 'pinecone', 'haystack', 'openai apis',\n",
    "    'langchain', 'llamaindex', 'embeddings',\n",
    "\n",
    "    # Algorithms and Data Structures\n",
    "    'bloom filter', 'count-min sketch', 'hyperloglog', 'hashing', 'sharding',\n",
    "    'partitioning', 'consistent hashing', 'merkle trees', 'trie', 'b-tree',\n",
    "    'skip lists', 'graph algorithms', 'pagerank', 'giraph', 'pregel', 'gelly',\n",
    "    'priority queues', 'heap', 'hash tables', 'dynamic programming',\n",
    "    'shortest path algorithms', 'a-star', 'dijkstra', 'kruskal', 'prim',\n",
    "    'suffix trees', 'trie structures', 'radix sort', 'red-black trees', 'quadtree',\n",
    "    'k-d tree', 'segment tree', 'fenwick tree', 'bit masking', 'splay tree',\n",
    "    'bloomier filters', 'min-heap', 'max-heap', 'hashmaps', 'sparse matrices',\n",
    "    'dense matrices', 'adjacency lists', 'minimax algorithm',\n",
    "    'monte carlo tree search', 'simulated annealing', 'tabu search',\n",
    "    'bellman-ford', 'convex hull', 'suffix arrays', 'lzw compression',\n",
    "    'sparse neural networks', 'bipartite graphs', 'euler paths',\n",
    "    'hamiltonian paths', 'approximation algorithms',\n",
    "\n",
    "    # Application Areas\n",
    "    'predictive analytics', 'natural language processing', 'computer vision',\n",
    "    'time series analysis', 'social network analysis', 'bioinformatics',\n",
    "    'recommender systems', 'internet of things', 'cybersecurity', 'financial analytics',\n",
    "    'stock market analysis', 'marketing analytics', 'big graph processing',\n",
    "    'geospatial analysis', 'sentiment analysis', 'digital health', 'personalization',\n",
    "    'audio analytics', 'robotics', 'autonomous vehicles', 'genomics', 'climate modeling',\n",
    "    'smart cities', 'energy optimization', 'fraud detection', 'e-commerce analytics',\n",
    "    'supply chain analytics', 'edge analytics', 'telecommunications analytics',\n",
    "    'healthcare analytics', 'environmental monitoring', 'disaster prediction',\n",
    "    'text summarization', 'translation systems', 'video analytics', 'speech recognition',\n",
    "    'image segmentation', 'retail analytics', 'transportation analytics',\n",
    "    'insurance analytics', 'gaming analytics', 'agrotech analytics',\n",
    "    'educational data mining', 'precision medicine', 'quantum computing',\n",
    "    'remote sensing', 'smart retail', '5g networks analytics', 'streaming media analytics',\n",
    "    'virtual reality data', 'blockchain data analysis',\n",
    "\n",
    "    # Concepts and Approaches\n",
    "    'etl', 'elt', 'stream processing', 'parallel computing', 'distributed systems',\n",
    "    'in-memory computing', 'lambda architecture', 'kappa architecture', 'cap theorem',\n",
    "    'base approach', 'acid transactions', 'horizontal scaling', 'vertical scaling',\n",
    "    'data governance', 'data quality', 'data modeling', 'semantic web', 'microservices',\n",
    "    'containerization', 'orchestration', 'event-driven architecture',\n",
    "    'real-time analytics', 'batch processing', 'stream analytics', 'stateful processing',\n",
    "    'stateless processing', 'low-latency processing', 'adaptive streaming',\n",
    "    'data wrangling', 'federated learning', 'data pipelines', 'data aggregation',\n",
    "    'event sourcing', 'data lakes', 'metadata management', 'data versioning',\n",
    "    'data lineage', 'data observability', 'composable data', 'decentralized data',\n",
    "    'dataops', 'edge computing', 'digital twins', 'neuro-symbolic ai',\n",
    "    'knowledge distillation',\n",
    "\n",
    "    # Data Visualization\n",
    "    'tableau', 'power bi', 'qlikview', 'd3.js', 'matplotlib', 'seaborn', 'plotly',\n",
    "    'ggplot2', 'kibana', 'grafana', 'superset', 'dash', 'bokeh', 'gephi',\n",
    "    'data storytelling', 'heatmaps', 'scatter plots', 'box plots', 'histograms',\n",
    "    'time series visualization', 'network visualization', 'interactive dashboards',\n",
    "    'infographics', 'sankey diagrams', 'word clouds', 'sunburst charts', 'radar charts',\n",
    "    'treemaps', 'correlation plots', 'observable plot', 'rawgraphs', 'flourish',\n",
    "    'highcharts', 'chart.js', 'piktochart', 'anychart', 'datawrapper', 'infogram',\n",
    "    'veusz', 'charticulator'\n",
    "\n",
    "    # Добавленные термины из domain_terms\n",
    "    'Big Data','large-scale data', 'Data Analysis', 'Data Mining', 'Data Science', 'Artificial Intelligence',\n",
    "    'MapReduce', 'NoSQL', 'SQL', 'Data Warehousing', 'Business Intelligence',\n",
    "    'Prescriptive Analytics', 'Descriptive Analytics', 'Apache Hive', 'Apache Pig',\n",
    "    'Apache Flink', 'Apache Storm', 'Apache Cassandra', 'HBase', 'PyTorch', 'Keras',\n",
    "    'Scikit-learn', 'Pandas', 'NumPy', 'Data Cleaning', 'Data Integration',\n",
    "    'AWS', 'Google Cloud Platform', 'Microsoft Azure', 'MLOps', 'Data Pipeline',\n",
    "    'OLAP', 'OLTP', 'Fog Computing', 'Feature Engineering', 'Dimensionality Reduction',\n",
    "    'Principal Component Analysis', 'Singular Value Decomposition', 'Data Anonymization',\n",
    "    'Privacy Preserving Data Mining', 'GDPR', 'Data Ethics', 'Database Management Systems',\n",
    "    'SQL Server', 'Teradata', 'Data Catalog', 'Data Lineage', 'Data Lakehouse',\n",
    "    'Structured Data', 'Unstructured Data', 'Semi-Structured Data', 'Distributed Computing',\n",
    "    'HDFS', 'Columnar Databases', 'Graph Databases', 'Neo4j', 'Data Preprocessing',\n",
    "    'Data Sampling', 'Data Imputation', 'Anomaly Detection', 'Recurrent Neural Networks',\n",
    "    'Long Short-Term Memory', 'Attention Mechanisms', 'Data Monetization', 'Data Strategy',\n",
    "    'Data Literacy', 'Data Democratization', 'Self-Service Analytics', 'Augmented Analytics',\n",
    "    'Explainable AI', 'AutoML', 'Synthetic Data', 'Data-Driven Decision Making',\n",
    "    'DataOps', 'Data Stewardship', 'Data Scientist', 'Data Engineer', 'Data Analyst',\n",
    "    'Data Architect', 'Chief Data Officer', 'Scalability', 'Throughput', 'Fault Tolerance',\n",
    "    'Load Balancing', 'API', 'RESTful Services', 'Parquet', 'Avro', 'ORC', 'Apache Arrow',\n",
    "    'Data Storage', 'Data Retrieval', 'ETL Tools', 'Informatica', 'Talend', 'Pentaho',\n",
    "    'SSIS', 'Data Enrichment', 'Data Warehouse Automation', 'Data Ingestion', 'Data Retention',\n",
    "    'Data Archiving', 'Data Lifecycle Management', 'Data Replication', 'Master Data Management',\n",
    "    'Reference Data', 'Data Provenance', 'Data Virtualization', 'Data Federation',\n",
    "    'Data Consolidation', 'Data Blending', 'Data Cubes', 'Data Mesh', 'Data Fabric',\n",
    "    'Data Tokenization', 'Data Security', 'Data Privacy', 'Data Loss Prevention',\n",
    "    'Access Control', 'Authentication', 'Authorization', 'Role-Based Access Control',\n",
    "    'Identity and Access Management', 'Encryption at Rest', 'Encryption in Transit',\n",
    "    'SSL', 'TLS', 'Single Sign-On', 'Data Compliance', 'HIPAA', 'PCI DSS', 'Data Breach',\n",
    "    'Data Incident Response', 'High Availability', 'Serverless Computing',\n",
    "    'Function as a Service', 'Platform as a Service', 'Infrastructure as a Service',\n",
    "    'Software as a Service', 'Apache Iceberg', 'Apache Hudi', 'CAP Theorem',\n",
    "    'Consistency', 'Availability', 'Partition Tolerance', 'ACID', 'BASE', 'CQRS',\n",
    "    'Message Queues', 'RabbitMQ', 'ActiveMQ', 'ZeroMQ', 'MQTT', 'Streaming Data',\n",
    "    'Kafka Streams', 'Kinesis', 'Pub/Sub', 'Event Hubs', 'Continuous Integration',\n",
    "    'Continuous Deployment', 'Agile Methodology', 'Scrum', 'Data Transformation',\n",
    "    'Data Normalization', 'Standardization', 'One-Hot Encoding', 'Label Encoding',\n",
    "    'Cross-Validation', 'Random Search', 'K-Fold Cross Validation', 'Ensemble Methods',\n",
    "    'Stacking', 'Transfer Learning', 'Model Deployment', 'Model Serving', 'Model Monitoring',\n",
    "    'A/B Testing', 'Concept Drift', 'Model Retraining', 'Model Explainability',\n",
    "    'SHAP Values', 'LIME', 'Partial Dependence Plots', 'Data Annotation', 'Labeling',\n",
    "    'Overfitting', 'Underfitting', 'Bias-Variance Tradeoff', 'Regularization',\n",
    "    'Elastic Net', 'Dropout', 'Early Stopping', 'Mini-Batch Gradient Descent',\n",
    "    'Adam Optimizer', 'Activation Functions', 'Sigmoid Function', 'Softmax Function',\n",
    "    'Cost Function', 'Loss Function', 'Backpropagation', 'Epoch', 'Batch Size',\n",
    "    'Data Silo', 'Edge AI', 'Data Compression', 'UMAP', 'Data Partitioning',\n",
    "    'Data Sharding', 'Bloom Filters', 'Data Streaming Algorithms', 'Real-Time Analytics',\n",
    "    'Apache Druid', 'ClickHouse', 'OLAP Cubes', 'Data Marketplace', 'Data Exchange',\n",
    "    'Data Brokerage', 'Data Licensing', 'Data Contracts', 'Data Quality Metrics',\n",
    "    'Data Accuracy', 'Data Completeness', 'Data Timeliness', 'Data Conformity',\n",
    "    'Data Uniqueness', 'Data Validation', 'Data Profiling', 'Data Observability',\n",
    "    'Data SLOs', 'Anonymization Techniques', 'K-Anonymity', 'Differential Privacy',\n",
    "    'Homomorphic Encryption', 'Secure Multi-Party Computation', 'Zero-Knowledge Proofs',\n",
    "    'Blockchain', 'Smart Contracts', 'Consensus Algorithms', 'Proof of Work',\n",
    "    'Proof of Stake', 'Machine Learning Lifecycle', 'Data Acquisition', 'Data Processing',\n",
    "    'Model Building', 'Model Evaluation', 'Model Maintenance', 'Experiment Tracking',\n",
    "    'Weights & Biases', 'Data Privacy Laws', 'CCPA', 'Data Residency', 'Hybrid Cloud',\n",
    "    'Multicloud', 'Data Compression Techniques', 'Video Analytics', 'Audio Analytics',\n",
    "    'Topic Modeling', 'Hierarchical Clustering', 'DBSCAN', 'Affinity Propagation',\n",
    "    'Mean Shift', 'K-Nearest Neighbors', 'Multilayer Perceptron', 'Polynomial Regression',\n",
    "    'Ridge Regression', 'Lasso Regression', 'Elastic Net Regression', 'ARIMA Models',\n",
    "    'Autocorrelation', 'Cross-Correlation', 'Spectral Analysis', 'Fourier Transform',\n",
    "    'Wavelets', 'Mutual Information', 'Chi-Square Test', 'Recursive Feature Elimination',\n",
    "    'Univariate Selection', 'Canonical Correlation Analysis', 'Discriminant Analysis',\n",
    "    'Cluster Analysis', 'Cohort Analysis', 'Customer Lifetime Value', 'Churn Prediction',\n",
    "    'Market Basket Analysis', 'Apriori Algorithm', 'FP-Growth Algorithm', 'Hybrid Recommenders',\n",
    "    'Matrix Factorization', 'Isolation Forest', 'One-Class SVM', 'Local Outlier Factor',\n",
    "    'CRISP-DM', 'SEMMA', 'KDD', 'Information Retrieval', 'Named Entity Recognition',\n",
    "    'Latent Dirichlet Allocation', 'Latent Semantic Analysis', 'Word Embeddings', 'GloVe',\n",
    "    'BERT', 'Autoencoders', 'Data Masking', 'Data Swapping', 'Re-identification Risk',\n",
    "    'DAMA-DMBOK', 'DCAM', 'Data Stewardship Council', 'Data Quality Tools',\n",
    "    'Data Integration Tools', 'Data Governance Tools', 'Data Catalog Tools',\n",
    "    'Data Lineage Tools', 'Data Masking Tools', 'Data Encryption Tools',\n",
    "    'Data Backup and Recovery Tools', 'Cloud Storage', 'Object Storage', 'Block Storage',\n",
    "    'File Storage', 'SAN', 'NAS', 'Storage Tiers', 'Cold Storage', 'Warm Storage',\n",
    "    'Data Retention Policies', 'Data Disposal', 'Data Destruction', 'Shredding',\n",
    "    'Degaussing', 'Physical Destruction', 'Data Forensics', 'Incident Response',\n",
    "    'SIEM', 'SOAR', 'Threat Intelligence', 'Vulnerability Management',\n",
    "    'Penetration Testing', 'Ethical Hacking', 'Security Awareness Training', 'Phishing',\n",
    "    'Malware', 'Ransomware', 'Security Monitoring', 'Logging', 'Auditing',\n",
    "    'Compliance Auditing', 'Regulatory Compliance', 'Data Legislation',\n",
    "    'Privacy Impact Assessment', 'Risk Management', 'Business Continuity Planning',\n",
    "    'Green Computing', 'Energy Efficiency', 'Thermal Management', 'Cooling Systems',\n",
    "    'Virtualization', 'Hypervisors', 'Virtual Machines', 'Containers',\n",
    "    'Cloud Native Applications', 'Workflow Management', 'Apache Airflow', 'Luigi',\n",
    "    'Azkaban', 'Data Flow', 'ETL vs ELT', 'Change Data Capture', 'Query Optimization',\n",
    "    'Cost-Based Optimization', 'Rule-Based Optimization', 'Data Formats', 'YAML',\n",
    "    'ProtoBuf', 'Thrift', 'Data Serialization', 'Data Deserialization', 'Marshaling',\n",
    "    'Unmarshaling', 'Operational Data Store', 'Staging Area', 'Sandbox Environment',\n",
    "    'Data Environments', 'Version Control', 'Branching Strategies', 'Trunk-Based Development',\n",
    "    'Code Review', 'Pull Requests', 'Merge Requests', 'Build Automation', 'Test Automation',\n",
    "    'Unit Testing', 'Integration Testing', 'System Testing', 'Acceptance Testing',\n",
    "    'Regression Testing', 'Test-Driven Development', 'Behavior-Driven Development',\n",
    "    'API Contracts', 'SLAs', 'SLOs', 'KPIs', 'Metrics', 'Monitoring', 'Alerting',\n",
    "    'Observability', 'Tracing', 'Metrics Collection', 'Dashboards', 'Reports',\n",
    "    'Choropleth Maps', 'Bar Charts', 'Line Charts', 'Pie Charts', 'Interactive Visualizations',\n",
    "    'Natural Language Query', 'Voice Interfaces', 'Chatbots', 'Virtual Assistants',\n",
    "    'Cognitive Computing', 'Knowledge Graphs', 'Ontologies', 'Taxonomies', 'RDF',\n",
    "    'SPARQL', 'OWL', 'Open Data', 'Data Sharing', 'FAIR Principles', 'Data Repositories',\n",
    "    'Data Archives', 'Data Curation', 'Data Lifecycle', 'Data Management Plan',\n",
    "    'Metadata Standards', 'Dublin Core', 'Data Citation', 'Data Journals', 'Open Science',\n",
    "    'Data Collaborations', 'Data Consortia', 'Data Partnerships', 'Data Commons',\n",
    "    'Data Cooperatives', 'Data Crowdsourcing', 'Citizen Science','algorithm'\n",
    "]\n",
    "\n",
    "lowercase_terms = [term.lower() for term in domain_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42a00eb6-fbcf-47ab-9b8c-1a6534330446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных из MongoDB...\n",
      "Всего объединенных документов: 86564\n",
      "Документы с указанным годом публикации: 86564\n",
      "Группировка текстов по категориям и годам...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86564/86564 [00:06<00:00, 13151.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проведение тематического моделирования...\n",
      "\n",
      "--- Анализ категории: ALGORITHM ---\n",
      "\n",
      "--- Анализ категории: TECHNOLOGY ---\n",
      "\n",
      "--- Анализ категории: METHOD ---\n",
      "\n",
      "--- Анализ категории: DATA ---\n",
      "\n",
      "--- Анализ категории: MODEL ---\n",
      "\n",
      "--- Анализ категории: APPLICATION ---\n",
      "\n",
      "--- Анализ категории: TASK ---\n",
      "\n",
      "--- Анализ категории: TOOL ---\n",
      "\n",
      "--- Анализ категории: FRAMEWORK ---\n",
      "\n",
      "--- Анализ категории: PARAMETER ---\n",
      "\n",
      "--- Анализ категории: METRIC ---\n",
      "\n",
      "Сохранение результатов...\n",
      "Результаты успешно сохранены.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_trends() got an unexpected keyword argument 'num_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 175\u001b[0m\n\u001b[0;32m    171\u001b[0m         plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m    172\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m--> 175\u001b[0m plot_trends(topic_trends, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_trends() got an unexpected keyword argument 'num_topics'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Подключение к MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "research_paper_collection = client['VKR1']['research_paper']\n",
    "ner_results_collection = client['VKR1']['ner_results']\n",
    "\n",
    "# Загрузка данных из MongoDB\n",
    "def load_data():\n",
    "    try:\n",
    "        print(\"Загрузка данных из MongoDB...\")\n",
    "        research_papers = pd.DataFrame(list(research_paper_collection.find({}, {\"_id\": 1, \"yearPublished\": 1})))\n",
    "        ner_results = pd.DataFrame(list(ner_results_collection.find({}, {\"_id\": 1, \"entities\": 1})))\n",
    "        return research_papers, ner_results\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке данных: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "research_papers, ner_results = load_data()\n",
    "\n",
    "# Проверка данных\n",
    "if research_papers.empty or ner_results.empty:\n",
    "    print(\"Нет данных для анализа. Проверьте подключение и содержимое коллекций MongoDB.\")\n",
    "    exit()\n",
    "\n",
    "# Объединение данных по _id\n",
    "merged_data = pd.merge(research_papers, ner_results, on=\"_id\")\n",
    "print(f\"Всего объединенных документов: {len(merged_data)}\")\n",
    "\n",
    "# Фильтрация документов с указанным годом публикации\n",
    "merged_data = merged_data[merged_data[\"yearPublished\"].notnull()]\n",
    "print(f\"Документы с указанным годом публикации: {len(merged_data)}\")\n",
    "\n",
    "# Шаг 1. Группировка текстов по категориям и годам\n",
    "categories = ['ALGORITHM', 'TECHNOLOGY', 'METHOD', 'DATA', 'MODEL', 'APPLICATION', 'TASK', 'TOOL', 'FRAMEWORK', 'PARAMETER', 'METRIC']\n",
    "tagged_corpus = {tag: {} for tag in categories}\n",
    "\n",
    "print(\"Группировка текстов по категориям и годам...\")\n",
    "for _, row in tqdm(merged_data.iterrows(), total=len(merged_data)):\n",
    "    year = row[\"yearPublished\"]\n",
    "    entities = row.get(\"entities\", [])\n",
    "    if not entities:\n",
    "        continue\n",
    "    for entity in entities:\n",
    "        tag = entity.get(\"label\")\n",
    "        text = entity.get(\"text\")\n",
    "        if tag in tagged_corpus and text:\n",
    "            tagged_corpus[tag].setdefault(year, []).append(text)\n",
    "\n",
    "# Шаг 2. Определение оптимального числа тем\n",
    "def find_optimal_topics(texts, vectorizer, min_topics=4, max_topics=15):\n",
    "    if len(texts) < 5:\n",
    "        return None, None, None, 0  # Недостаточно данных для анализа\n",
    "    bow_matrix = vectorizer.fit_transform(texts)\n",
    "    best_lda = None\n",
    "    best_topics = None\n",
    "    best_coherence = -np.inf\n",
    "    best_num_topics = 0\n",
    "\n",
    "    for num_topics in range(min_topics, max_topics + 1):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=num_topics,\n",
    "            max_iter=20,\n",
    "            random_state=42,\n",
    "            learning_method=\"batch\"\n",
    "        )\n",
    "        lda.fit(bow_matrix)\n",
    "        coherence = calculate_coherence(lda)\n",
    "        if coherence > best_coherence:\n",
    "            best_coherence = coherence\n",
    "            best_lda = lda\n",
    "            best_num_topics = num_topics\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            best_topics = {\n",
    "                f\"Topic {i}\": [\n",
    "                    (feature_names[j], topic[j]) for j in topic.argsort()[:-11:-1]\n",
    "                ]\n",
    "                for i, topic in enumerate(lda.components_)\n",
    "            }\n",
    "    return best_lda, vectorizer, best_topics, best_num_topics\n",
    "\n",
    "def calculate_coherence(model):\n",
    "    \"\"\"\n",
    "    Вычисляет когерентность как среднюю плотность тем.\n",
    "    \"\"\"\n",
    "    topic_word_distribution = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    coherence = np.mean(np.max(topic_word_distribution, axis=1))\n",
    "    return coherence\n",
    "\n",
    "# Шаг 3. Тематическое моделирование по годам\n",
    "def perform_topic_modeling():\n",
    "    topic_trends = {tag: {} for tag in categories}\n",
    "    topic_keywords = {tag: {} for tag in categories}\n",
    "    optimal_topics_count = {tag: {} for tag in categories}\n",
    "\n",
    "    print(\"Проведение тематического моделирования...\")\n",
    "    for tag, yearly_texts in tagged_corpus.items():\n",
    "        print(f\"\\n--- Анализ категории: {tag} ---\")\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            min_df=2,\n",
    "            max_df=0.85\n",
    "        )\n",
    "\n",
    "        for year, texts in yearly_texts.items():\n",
    "            lda, vec, topics, num_topics = find_optimal_topics(texts, vectorizer)\n",
    "            if lda is not None:\n",
    "                topic_distribution = lda.transform(vec.transform(texts)).mean(axis=0)\n",
    "                topic_trends[tag][year] = topic_distribution.tolist()  # Преобразование для сериализации\n",
    "                topic_keywords[tag][year] = topics\n",
    "                optimal_topics_count[tag][year] = num_topics\n",
    "\n",
    "    return topic_trends, topic_keywords, optimal_topics_count\n",
    "\n",
    "topic_trends, topic_keywords, optimal_topics_count = perform_topic_modeling()\n",
    "\n",
    "# Шаг 4. Сохранение результатов\n",
    "def save_results(topic_trends, topic_keywords, optimal_topics_count):\n",
    "    print(\"\\nСохранение результатов...\")\n",
    "    try:\n",
    "        with open(\"topic_trends.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(topic_trends, f, indent=4)\n",
    "        with open(\"topic_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(topic_keywords, f, indent=4)\n",
    "        with open(\"optimal_topics_count.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(optimal_topics_count, f, indent=4)\n",
    "        print(\"Результаты успешно сохранены.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении результатов: {e}\")\n",
    "\n",
    "save_results(topic_trends, topic_keywords, optimal_topics_count)\n",
    "\n",
    "# Шаг 5. Визуализация трендов\n",
    "def plot_trends(topic_trends, optimal_topics_count):\n",
    "    \"\"\"\n",
    "    Визуализация распределения тем по годам.\n",
    "    \"\"\"\n",
    "    for tag, yearly_trends in topic_trends.items():\n",
    "        if not yearly_trends:\n",
    "            continue\n",
    "\n",
    "        # Преобразуем данные в DataFrame\n",
    "        df_trends = pd.DataFrame.from_dict(yearly_trends, orient=\"index\")\n",
    "        df_trends.sort_index(inplace=True)\n",
    "\n",
    "        # Если число топиков варьируется, обрезаем или дополняем недостающие значения\n",
    "        max_topics = max(optimal_topics_count[tag].values())\n",
    "        df_trends = df_trends.apply(lambda row: row.tolist() + [0] * (max_topics - len(row)), axis=1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for topic_idx in range(max_topics):\n",
    "            plt.plot(\n",
    "                df_trends.index,\n",
    "                df_trends.apply(lambda x: x[topic_idx], axis=1),\n",
    "                label=f\"Topic {topic_idx}\"\n",
    "            )\n",
    "        plt.title(f\"Тренды тем по категории: {tag}\")\n",
    "        plt.xlabel(\"Год\")\n",
    "        plt.ylabel(\"Средняя вероятность темы\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_trends(topic_trends, num_topics=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d2436-8f8c-45b7-9017-f6fd9d468b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
