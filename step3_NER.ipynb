{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e9e60-4e21-4508-9641-36a466d49114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Загрузка научной модели SciSpacy\n",
    "# nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import tqdm\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70046d89-6abd-4e8f-870e-b8badadd1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bcd2f-cd3e-4186-a4ed-6051c6e7b06a",
   "metadata": {},
   "source": [
    "# Создание размеченного датасета для дообучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92878710-ac86-4c16-b549-a8940c3b19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"TECHNOLOGY\": [\n",
    "        'artificial intelligence', 'big data', 'internet of things (IoT)', \n",
    "        'cloud computing', 'quantum computing', 'edge computing', \n",
    "        'blockchain', '5g networks', 'augmented reality', 'virtual reality', \n",
    "        'serverless computing', 'digital twins', 'microservices', \n",
    "        'event-driven architecture', 'in-memory computing', 'fog computing', \n",
    "        'cyber-physical systems', 'software-defined networks', \n",
    "        'data lakehouse', 'data mesh', 'robotics process automation (RPA)',\n",
    "        'human-computer interaction (HCI)', 'cybersecurity', 'distributed computing', \n",
    "        'neuro-symbolic AI', 'federated learning', 'privacy-preserving AI',\n",
    "        'explainable AI (XAI)', 'multi-cloud environments', 'data observability',\n",
    "        'green computing', 'energy-efficient AI', 'smart contracts',\n",
    "        'autonomous systems', 'immersive technologies', 'zero-trust architecture',\n",
    "        'context-aware computing', 'haptic technologies', 'nanotechnologies'\n",
    "    ],\n",
    "    \"METHOD\": [\n",
    "        'principal component analysis (PCA)', 'bayesian methods', \n",
    "        'markov chains', 'gradient boosting', 'stochastic processes', \n",
    "        'time series decomposition', 'dynamic programming', \n",
    "        'feature engineering', 'dimensionality reduction', \n",
    "        'data sampling', 'data imputation', 'data augmentation', \n",
    "        'semi-supervised learning', 'self-supervised learning', \n",
    "        'meta-learning', 'few-shot learning', 'multi-task learning', \n",
    "        'transfer learning', 'ensemble learning', \n",
    "        'autoregressive integrated moving average (ARIMA)', \n",
    "        'exponential smoothing', 'kalman filtering', 'survival analysis', \n",
    "        'probabilistic graphical models', 'causal inference', \n",
    "        'topic modeling', 'outlier detection', 'active learning', \n",
    "        'neural architecture search', 'attention mechanisms', \n",
    "        'hierarchical clustering', 'contrastive learning',\n",
    "        'adaptive boosting', 'bagging', 'maximum likelihood estimation',\n",
    "        'expectation-maximization', 'manifold learning', 'spectral embedding',\n",
    "        'regularization techniques', 'robust optimization', \n",
    "        'decision analysis', 'swarm intelligence'\n",
    "    ],\n",
    "    \"ALGORITHM\": [\n",
    "        'k-means', 'decision trees', 'random forest', 'svm', \n",
    "        'naive bayes', 'knn', 'logistic regression', 'linear regression', \n",
    "        'genetic algorithms', 'gradient descent', 'simulated annealing', \n",
    "        'particle swarm optimization', 'hill climbing', 'pagerank', \n",
    "        'dijkstra algorithm', 'kruskal algorithm', 'prim algorithm', \n",
    "        'bipartite matching', 'shortest path algorithms', \n",
    "        'hierarchical clustering', 'dbscan', 'mean-shift clustering', \n",
    "        'spectral clustering', 'xgboost', 'lightgbm', 'catboost', \n",
    "        'deep q-learning', 'ppo (proximal policy optimization)', \n",
    "        'transformer models', 'variational inference', \n",
    "        'reinforcement learning algorithms', 'apriori', 'fp-growth', \n",
    "        'monte carlo tree search', 'long short-term memory networks (LSTM)',\n",
    "        'convolutional neural networks (CNN)', 'self-organizing maps', \n",
    "        'word2vec', 'doc2vec', 'collaborative filtering algorithms', \n",
    "        'temporal difference learning', 'value iteration', 'policy gradient methods',\n",
    "        'trust region policy optimization (TRPO)', 'asynchronous advantage actor-critic (A3C)'\n",
    "    ],\n",
    "    \"TASK\": [\n",
    "        'classification', 'clustering', 'regression', \n",
    "        'dimensionality reduction', 'time series forecasting', \n",
    "        'anomaly detection', 'sentiment analysis', 'recommendation systems', \n",
    "        'topic modeling', 'translation', 'object detection', \n",
    "        'speech recognition', 'image segmentation', 'text summarization', \n",
    "        'document classification', 'entity recognition', \n",
    "        'causal modeling', 'explainable AI', 'automated feature extraction',\n",
    "        'intelligent tutoring', 'robot path planning', 'game AI design', \n",
    "        'event detection', 'sequence labeling', 'action recognition',\n",
    "        'domain adaptation', 'knowledge graph construction'\n",
    "    ],\n",
    "    \"MODEL\": [\n",
    "        'linear regression', 'logistic regression', 'decision trees', \n",
    "        'random forest', 'k-nearest neighbors (kNN)', 'support vector machine (SVM)', \n",
    "        'naive bayes', 'multilayer perceptron (MLP)', 'convolutional neural networks (CNN)', \n",
    "        'recurrent neural networks (RNN)', 'transformers', 'bert', \n",
    "        'gpt', 'longformer', 'albert', 'graph neural networks (GNN)', \n",
    "        'autoencoders', 'generative adversarial networks (GAN)', \n",
    "        'tabular neural networks', 'variational autoencoders (VAE)', \n",
    "        'language models', 'vision transformers (ViT)', 'ensemble models', \n",
    "        'mixture of experts', 'meta-modeling', 'neuro-symbolic models',\n",
    "        'zero-shot models', 'contrastive learning models', 'sequence-to-sequence models',\n",
    "        'multi-modal transformers', 'attention-based models', 'biLSTMs', 'capsule networks'\n",
    "    ],\n",
    "    \"TOOL\": [\n",
    "        'apache hadoop', 'apache spark', 'tensorflow', 'pytorch', \n",
    "        'scikit-learn', 'pandas', 'numpy', 'matplotlib', \n",
    "        'seaborn', 'plotly', 'd3.js', 'tableau', 'power bi', \n",
    "        'kafka', 'apache beam', 'apache flink', 'kubeflow', \n",
    "        'mlflow', 'grafana', 'superset', 'aws glue', 'databricks', \n",
    "        'nltk', 'spacy', 'gensim', 'flask', 'streamlit', \n",
    "        'fastapi', 'transformers library', 'huggingface datasets', \n",
    "        'langchain', 'ray tune', 'azure ml', 'google colab', \n",
    "        'anaconda', 'mlpack', 'jupyter lab', 'knime',\n",
    "        'snowflake', 'airflow', 'datadog', 'matplotlib', 'dash', \n",
    "        'highcharts', 'supervised library', 'mlxtend'\n",
    "    ],\n",
    "    \"FRAMEWORK\": [\n",
    "        'tensorflow', 'pytorch', 'keras', 'apache flink', \n",
    "        'apache storm', 'ray', 'dask', 'spark mllib', \n",
    "        'scikit-learn pipelines', 'fastai', 'hugging face transformers', \n",
    "        'langchain', 'ray tune', 'azure ml', 'google ai platform', \n",
    "        'kubeflow pipelines', 'mlflow tracking', 'microsoft synapse', \n",
    "        'amazon sagemaker', 'openvino', 'mlxtend', 'bigdl', \n",
    "        'deeplearning4j', 'onnx runtime', 'paddlepaddle'\n",
    "    ],\n",
    "    \"DATA\": [\n",
    "        'structured data', 'unstructured data', 'semi-structured data', \n",
    "        'time series data', 'historical data', 'geospatial data', \n",
    "        'streaming data', 'synthetic data', 'metadata', \n",
    "        'bigquery datasets', 'delta lake datasets', 'data cubes', \n",
    "        'training data', 'test data', 'validation data', \n",
    "        'annotated datasets', 'graph data', 'relational data', \n",
    "        'vector embeddings', 'text corpora', 'speech data', \n",
    "        'event streams', 'tabular data', 'spatial-temporal datasets',\n",
    "        'encrypted datasets', 'multi-view data'\n",
    "    ],\n",
    "    \"PARAMETER\": [\n",
    "        'learning rate', 'batch size', 'number of layers', \n",
    "        'dropout rate', 'number of epochs', 'activation functions', \n",
    "        'optimizer settings', 'momentum', 'weight decay', \n",
    "        'regularization parameters', 'embedding size', \n",
    "        'sequence length', 'window size', 'attention heads', \n",
    "        'hidden layer size', 'decay rate', 'max iterations', \n",
    "        'gradient clipping', 'loss function', 'hyperparameter ranges', \n",
    "        'convergence thresholds'\n",
    "    ],\n",
    "    \"METRIC\": [\n",
    "        'accuracy', 'precision', 'recall', 'f1-score', \n",
    "        'mean squared error (MSE)', 'root mean squared error (RMSE)', \n",
    "        'mean absolute error (MAE)', 'roc-auc score', 'log loss', \n",
    "        'silhouette score', 'calinski-harabasz index', 'perplexity', \n",
    "        'bleu score', 'rouge score', 'edit distance', \n",
    "        'cosine similarity', 'mean average precision (MAP)', \n",
    "        'dcg', 'ndcg', 'kendall rank correlation', 'pairwise precision', \n",
    "        'normalized mutual information (NMI)', 'jaccard index'\n",
    "    ],\n",
    "    \"APPLICATION\": [\n",
    "        'predictive analytics', 'healthcare analytics', 'genomics', \n",
    "        'fraud detection', 'marketing analytics', 'recommender systems', \n",
    "        'financial analytics', 'cybersecurity', 'traffic prediction', \n",
    "        'robotics', 'autonomous vehicles', 'smart cities', \n",
    "        'natural language processing', 'time series forecasting', \n",
    "        'climate modeling', 'bioinformatics', 'customer lifetime value', \n",
    "        'gaming analytics', 'personalization systems', 'geospatial analysis', \n",
    "        'speech synthesis', 'virtual assistants', 'real-time analytics', \n",
    "        'demand forecasting', 'inventory optimization', \n",
    "        'e-commerce analytics', 'social media analytics', \n",
    "        'precision agriculture', 'renewable energy optimization', \n",
    "        'disaster management', 'financial fraud detection', 'edge AI applications'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89747b31-6c6e-4f0b-9210-904f4b6528da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  шаблоны предложений\n",
    "templates = [\n",
    "    \"The study demonstrates how {} and {} can improve the efficiency of {} tasks.\",\n",
    "    \"Using {}, the authors were able to achieve significant improvements in {} with metrics like {}.\",\n",
    "    \"{} and {} are critical for {} and have been explored in depth in this research.\",\n",
    "    \"This paper compares {} and {} for solving problems related to {}.\",\n",
    "    \"Advanced techniques such as {} and {} have been utilized in applications like {}.\",\n",
    "    \"The experimental setup involved using {} for tasks like {}, achieving notable {}.\",\n",
    "    \"Recent advancements in {} have opened new possibilities for improving {}.\",\n",
    "    \"{} has been combined with {} to create innovative solutions for {}.\",\n",
    "    \"In the context of {}, methods such as {} and {} were extensively evaluated.\",\n",
    "    \"Key contributions include the integration of {} with {} for enhanced {}.\",\n",
    "    \"The authors present a novel approach combining {} and {} to tackle challenges in {}.\",\n",
    "    \"Performance improvements were observed when {} was applied alongside {} for {}.\",\n",
    "    \"The methodology employs {} and {} for robust {} pipelines.\",\n",
    "    \"This approach highlights the synergy between {} and {} in enhancing {} outcomes.\",\n",
    "    \"Case studies demonstrate the practical applications of {} and {} in {} scenarios.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51de08-ae47-4ce0-89f6-0b30ae68a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_terms = [term.lower() for term in domain_terms] from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class DomainWeightedTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, domain_terms=None, domain_weight=2, **kwargs):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного TfidfVectorizer.\n",
    "        :param domain_terms: Список или множество доменных терминов.\n",
    "        :param domain_weight: Вес, на который умножаются доменные термины.\n",
    "        :param kwargs: Остальные параметры TfidfVectorizer.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_terms = set(domain_terms) if domain_terms else set()\n",
    "        self.domain_weight = domain_weight\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Преобразование данных в матрицу признаков с увеличением веса доменных терминов.\n",
    "        \"\"\"\n",
    "        X_tfidf = super().transform(X)\n",
    "        if not self.domain_terms:\n",
    "            return X_tfidf\n",
    "        \n",
    "        # Получение индексов доменных терминов\n",
    "        feature_names = np.array(self.get_feature_names_out())\n",
    "        domain_indices = [i for i, term in enumerate(feature_names) if term in self.domain_terms]\n",
    "\n",
    "        # Увеличение весов для доменных терминов\n",
    "        weights = np.ones(X_tfidf.shape[1])\n",
    "        weights[domain_indices] *= self.domain_weight\n",
    "\n",
    "        # Применение весов\n",
    "        return X_tfidf.multiply(weights)\n",
    "\n",
    "# Использование кастомного векторизатора\n",
    "vectorizer = DomainWeightedTfidfVectorizer(domain_terms=domain_terms, domain_weight=2, stop_words='english')\n",
    "standard_stopwords = list(stopwords.words('english'))\n",
    "additional_stopwords = ['study','research','phd','use','university','abstract','published','thesis','paper','data','using','used']\n",
    "full_stopwords = standard_stopwords + additional_stopwords\n",
    "\n",
    "vectorizer_2 = DomainWeightedTfidfVectorizer(\n",
    "    domain_terms=lowercase_terms,\n",
    "    domain_weight=10,\n",
    "    stop_words=full_stopwords,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=5000,\n",
    "    max_df=0.9,  # Расширяем диапазон\n",
    "    min_df=5  # Учитываем больше редких слов\n",
    ") # 1. Функция для настройки UMAP\n",
    "def configure_umap(n_neighbors=50, n_components=5, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Настройка модели UMAP для уменьшения размерности.\n",
    "    :param n_neighbors: Количество соседей для локального графа.\n",
    "    :param n_components: Размерность выходного пространства.\n",
    "    :param metric: Метрика расстояния.\n",
    "    :return: Настроенный объект UMAP.\n",
    "    \"\"\"\n",
    "    return UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        metric=metric,\n",
    "        random_state=42  # Фиксация для воспроизводимости\n",
    "    )\n",
    "\n",
    "# 2. Функция для настройки HDBSCAN\n",
    "def configure_hdbscan(min_cluster_size=10, min_samples=1, metric='euclidean', \n",
    "                      cluster_selection_method='leaf', cluster_selection_epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Настройка модели HDBSCAN для кластеризации.\n",
    "    :param min_cluster_size: Минимальный размер кластера.\n",
    "    :param min_samples: Минимальное число точек для плотности.\n",
    "    :param metric: Метрика расстояния.\n",
    "    :param cluster_selection_method: Метод выбора кластеров ('eom' или 'leaf').\n",
    "    :param cluster_selection_epsilon: Допустимая разреженность кластеров. Чем больше значение, тем больше объединённых кластеров.\n",
    "    :return: Настроенный объект HDBSCAN.\n",
    "    \"\"\"\n",
    "    return hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,  # Установка epsilon\n",
    "        prediction_data=True  # Для работы с новыми данными\n",
    "    )\n",
    "\n",
    "# 3. Функция для уменьшения размерности эмбеддингов\n",
    "def reduce_embeddings(embeddings, n_components=50):\n",
    "    \"\"\"\n",
    "    Уменьшает размерность эмбеддингов с помощью PCA.\n",
    "    :param embeddings: Исходные эмбеддинги.\n",
    "    :param n_components: Количество компонентов PCA.\n",
    "    :return: Эмбеддинги с уменьшенной размерностью.\n",
    "    \"\"\"\n",
    "    pca_model = PCA(n_components=n_components, random_state=42)\n",
    "    return pca_model.fit_transform(embeddings)  # Настройка компонентов\n",
    "umap_model = configure_umap(n_neighbors=15, n_components=5, metric='cosine')\n",
    "hdbscan_model = configure_hdbscan(min_cluster_size=30, min_samples=20, metric='euclidean',cluster_selection_epsilon=0.25)\n",
    "embeddings_reduced = reduce_embeddings(embeddings_SCIBERT, n_components=12)\n",
    "\n",
    "# Создание модели BERTopic\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_2, umap_model=umap_model,hdbscan_model=hdbscan_model,)\n",
    "\n",
    "# Обучение модели на предобработанных текстах с вашими эмбеддингами\n",
    "topics, probs = topic_model.fit_transform(documents, embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66016f50-e8a1-4540-9b84-b0923d193474",
   "metadata": {},
   "source": [
    "# Процесс дообучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7914bc-18bd-4cce-8c0c-acf95c6304a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ekate\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Train size: 8000, Test size: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de2791b97e458a801052de8ef731e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade33b77a36c4d108cad382ca42eeb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Шаг 1. Загрузка синтетического размеченного датасета\n",
    "with open(\"synthetic_ner_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    synthetic_data = json.load(f)\n",
    "\n",
    "# Преобразование в формат Hugging Face\n",
    "def convert_to_hf_format(data):\n",
    "    tokens = [entry[\"tokens\"] for entry in data]\n",
    "    ner_tags = [entry[\"ner_tags\"] for entry in data]\n",
    "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
    "\n",
    "hf_data = convert_to_hf_format(synthetic_data)\n",
    "\n",
    "# Создание Dataset\n",
    "dataset = Dataset.from_dict(hf_data)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Шаг 2. Токенизация\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Уникальные тэги\n",
    "unique_labels = list(set(tag for tags in hf_data[\"ner_tags\"] for tag in tags))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Токенизация и выравнивание меток\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",  # Добавляем автоматическое выравнивание\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Связь токенов и слов\n",
    "        aligned_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)  # Игнорируем специальные токены ([CLS], [SEP])\n",
    "            else:\n",
    "                aligned_labels.append(label2id[label[word_id]])\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Шаг 3. Загрузка модели\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Шаг 4. Метрики оценки\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = predictions.argmax(-1)\n",
    "    true_labels = [\n",
    "        [id2label[label] for label in label_seq if label != -100]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred_seq, label_seq) if l != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Шаг 5. DataCollator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Используем смешанную точность\n",
    ")\n",
    "\n",
    "# Шаг 7. Обучение с использованием Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf33b341-3e93-4808-a0ff-6f5b2bc27ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 5:34:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.1377083191871643, metrics={'train_runtime': 20081.8453, 'train_samples_per_second': 0.398, 'train_steps_per_second': 0.05, 'total_flos': 2090770931712000.0, 'train_loss': 0.1377083191871643, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6c856a-2689-4f82-b9bb-e45ff31aec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Шаг 8. Сохранение модели\n",
    "trainer.save_model(\"./trained_scibert_ner_model\")\n",
    "tokenizer.save_pretrained(\"./trained_scibert_ner_model\")\n",
    "\n",
    "print(\"Model training completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e340d2-a25c-4abf-8181-b65b1c956173",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc69c3f1-dcd2-479e-a769-db069abf0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Путь к сохранённой модели\n",
    "model_path = \"./trained_scibert_ner_model\"\n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e69b75-923b-4233-946d-5061de7e3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: had, Label: DATA, Score: 0.5473\n",
      "Entity: ##oop, Label: DATA, Score: 0.5225\n",
      "Entity: apache spark, Label: TOOL, Score: 0.9924\n",
      "Entity: distributed computing, Label: TECHNOLOGY, Score: 0.9979\n",
      "Entity: principal component analysis, Label: METHOD, Score: 0.9988\n",
      "Entity: gradient boosting, Label: METHOD, Score: 0.9987\n",
      "Entity: data, Label: APPLICATION, Score: 0.6238\n",
      "Entity: processing, Label: METHOD, Score: 0.3217\n"
     ]
    }
   ],
   "source": [
    "# Создание NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "# Пример текста для тестирования\n",
    "\n",
    "\n",
    "# Разметка текста\n",
    "ner_results = ner_pipeline(example_text)\n",
    "\n",
    "# Печать результатов\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb6849-e36d-497f-9df2-563df4b99f08",
   "metadata": {},
   "source": [
    "# NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abe70f-351c-43b3-8baa-4ca54dae0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "collection = client['VKR1']['proccessed_for_topic_modelling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5ca2cbf-fee1-4222-93c4-bb8ab8bd4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список доменных терминов (можно расширить)\n",
    "domain_terms = [\n",
    "    # Big Data Technologies and Platforms\n",
    "    'hadoop', 'spark', 'flink', 'storm', 'kafka', 'hive', 'pig', 'cassandra',\n",
    "    'hbase', 'mongodb', 'redis', 'elasticsearch', 'solr', 'apache beam',\n",
    "    'apache samza', 'apache apex', 'presto', 'apache drill', 'apache tez',\n",
    "    'kudu', 'druid', 'nifi', 'zookeeper', 'oozie', 'mahout', 'sqoop', 'ambari',\n",
    "    'zeppelin', 'mesos', 'alluxio', 'yarn', 'mapreduce', 'azure hdinsight',\n",
    "    'amazon emr', 'google cloud dataproc', 'cloudera', 'hortonworks', 'databricks',\n",
    "    'clickhouse', 'apache ignite', 'azure synapse', 'dataproc', 'data fusion',\n",
    "    'bigquery', 'snowflake', 'redshift', 'minio', 'delta lake', 'lakehouse',\n",
    "    'trino', 'apache kylin', 'apache carbondata', 'hudi', 'iceberg', 'dremio',\n",
    "    'bigtable', 'open tsdb', 'prometheus', 'grafana loki', 'vectorized io',\n",
    "    'openwhisk', 'kubeflow', 'mlflow', 'apache pulsar', 'apache superset',\n",
    "    'apache ranger', 'datadog', 'new relic', 'dynatrace', 'splunk', 'logstash',\n",
    "    'fluentd', 'apache arrow', 'delta sharing', 'microsoft fabric', 'aws glue',\n",
    "    'athena', 'databricks unity catalog', 'citus', 'greenplum', 'hypertable',\n",
    "    'apache pinot', 'apache heron', 'hazelcast', 'couchbase','ai',\n",
    "\n",
    "    # Programming Languages and Tools\n",
    "    'python', 'scala', 'java', 'r', 'sql', 'nosql', 'julia', 'matlab', 'sas',\n",
    "    'perl', 'go', 'c++', 'rust', 'haskell', 'pig latin', 'hiveql', 'bash',\n",
    "    'shell scripting', 'php', 'typescript', 'javascript', 'groovy', 'terraform',\n",
    "    'ansible', 'ruby', 'fortran', 'erlang', 'f#', 'kotlin', 'elixir', 'nim',\n",
    "    'clojure', 'prolog', 'scheme', 'smalltalk', 'powershell', 'visual basic',\n",
    "    'tcl', 'awk', 'sed', 'dart', 'cobol', 'vbscript', 'actionscript', 'apl',\n",
    "    'postgresql', 'mariadb', 'google bigquery sql', 'pl/pgsql', 'solidity',\n",
    "\n",
    "    # Machine Learning and AI\n",
    "    'machine learning', 'deep learning', 'clustering', 'classification',\n",
    "    'regression', 'association rules', 'k-means', 'random forest',\n",
    "    'gradient boosting', 'svm', 'neural networks', 'decision trees',\n",
    "    'bayesian methods', 'markov chains', 'genetic algorithms', 'reinforcement learning',\n",
    "    'exponential smoothing', 'pca', 't-sne', 'lda', 'linear regression',\n",
    "    'logistic regression', 'naive bayes', 'knn', 'deep neural networks', 'rnn',\n",
    "    'cnn', 'autoencoders', 'transformers', 'attention mechanism', 'seq2seq models',\n",
    "    'ensemble learning', 'bagging', 'boosting', 'xgboost', 'lightgbm', 'catboost',\n",
    "    'hyperparameter tuning', 'grid search', 'bayesian optimization',\n",
    "    'self-supervised learning', 'contrastive learning', 'meta-learning',\n",
    "    'few-shot learning', 'multi-task learning', 'generative adversarial networks',\n",
    "    'variational autoencoders', 'graph neural networks', 'language models',\n",
    "    'bpe tokenization', 'word2vec', 'fasttext', 'doc2vec', 'sentence transformers',\n",
    "    'roberta', 'gpt', 'scibert', 'longformer', 'xlm', 'electra', 'albert',\n",
    "    'vision transformers (vit)', 'bertology', 'zero-shot learning',\n",
    "    'one-shot learning', 'distillation', 'federated gans', 'tabnet', 'deepar',\n",
    "    'fastai', 'hugging face transformers', 'pinecone', 'haystack', 'openai apis',\n",
    "    'langchain', 'llamaindex', 'embeddings',\n",
    "\n",
    "    # Algorithms and Data Structures\n",
    "    'bloom filter', 'count-min sketch', 'hyperloglog', 'hashing', 'sharding',\n",
    "    'partitioning', 'consistent hashing', 'merkle trees', 'trie', 'b-tree',\n",
    "    'skip lists', 'graph algorithms', 'pagerank', 'giraph', 'pregel', 'gelly',\n",
    "    'priority queues', 'heap', 'hash tables', 'dynamic programming',\n",
    "    'shortest path algorithms', 'a-star', 'dijkstra', 'kruskal', 'prim',\n",
    "    'suffix trees', 'trie structures', 'radix sort', 'red-black trees', 'quadtree',\n",
    "    'k-d tree', 'segment tree', 'fenwick tree', 'bit masking', 'splay tree',\n",
    "    'bloomier filters', 'min-heap', 'max-heap', 'hashmaps', 'sparse matrices',\n",
    "    'dense matrices', 'adjacency lists', 'minimax algorithm',\n",
    "    'monte carlo tree search', 'simulated annealing', 'tabu search',\n",
    "    'bellman-ford', 'convex hull', 'suffix arrays', 'lzw compression',\n",
    "    'sparse neural networks', 'bipartite graphs', 'euler paths',\n",
    "    'hamiltonian paths', 'approximation algorithms',\n",
    "\n",
    "    # Application Areas\n",
    "    'predictive analytics', 'natural language processing', 'computer vision',\n",
    "    'time series analysis', 'social network analysis', 'bioinformatics',\n",
    "    'recommender systems', 'internet of things', 'cybersecurity', 'financial analytics',\n",
    "    'stock market analysis', 'marketing analytics', 'big graph processing',\n",
    "    'geospatial analysis', 'sentiment analysis', 'digital health', 'personalization',\n",
    "    'audio analytics', 'robotics', 'autonomous vehicles', 'genomics', 'climate modeling',\n",
    "    'smart cities', 'energy optimization', 'fraud detection', 'e-commerce analytics',\n",
    "    'supply chain analytics', 'edge analytics', 'telecommunications analytics',\n",
    "    'healthcare analytics', 'environmental monitoring', 'disaster prediction',\n",
    "    'text summarization', 'translation systems', 'video analytics', 'speech recognition',\n",
    "    'image segmentation', 'retail analytics', 'transportation analytics',\n",
    "    'insurance analytics', 'gaming analytics', 'agrotech analytics',\n",
    "    'educational data mining', 'precision medicine', 'quantum computing',\n",
    "    'remote sensing', 'smart retail', '5g networks analytics', 'streaming media analytics',\n",
    "    'virtual reality data', 'blockchain data analysis',\n",
    "\n",
    "    # Concepts and Approaches\n",
    "    'etl', 'elt', 'stream processing', 'parallel computing', 'distributed systems',\n",
    "    'in-memory computing', 'lambda architecture', 'kappa architecture', 'cap theorem',\n",
    "    'base approach', 'acid transactions', 'horizontal scaling', 'vertical scaling',\n",
    "    'data governance', 'data quality', 'data modeling', 'semantic web', 'microservices',\n",
    "    'containerization', 'orchestration', 'event-driven architecture',\n",
    "    'real-time analytics', 'batch processing', 'stream analytics', 'stateful processing',\n",
    "    'stateless processing', 'low-latency processing', 'adaptive streaming',\n",
    "    'data wrangling', 'federated learning', 'data pipelines', 'data aggregation',\n",
    "    'event sourcing', 'data lakes', 'metadata management', 'data versioning',\n",
    "    'data lineage', 'data observability', 'composable data', 'decentralized data',\n",
    "    'dataops', 'edge computing', 'digital twins', 'neuro-symbolic ai',\n",
    "    'knowledge distillation',\n",
    "\n",
    "    # Data Visualization\n",
    "    'tableau', 'power bi', 'qlikview', 'd3.js', 'matplotlib', 'seaborn', 'plotly',\n",
    "    'ggplot2', 'kibana', 'grafana', 'superset', 'dash', 'bokeh', 'gephi',\n",
    "    'data storytelling', 'heatmaps', 'scatter plots', 'box plots', 'histograms',\n",
    "    'time series visualization', 'network visualization', 'interactive dashboards',\n",
    "    'infographics', 'sankey diagrams', 'word clouds', 'sunburst charts', 'radar charts',\n",
    "    'treemaps', 'correlation plots', 'observable plot', 'rawgraphs', 'flourish',\n",
    "    'highcharts', 'chart.js', 'piktochart', 'anychart', 'datawrapper', 'infogram',\n",
    "    'veusz', 'charticulator'\n",
    "\n",
    "    # Добавленные термины из domain_terms\n",
    "    'Big Data','large-scale data', 'Data Analysis', 'Data Mining', 'Data Science', 'Artificial Intelligence',\n",
    "    'MapReduce', 'NoSQL', 'SQL', 'Data Warehousing', 'Business Intelligence',\n",
    "    'Prescriptive Analytics', 'Descriptive Analytics', 'Apache Hive', 'Apache Pig',\n",
    "    'Apache Flink', 'Apache Storm', 'Apache Cassandra', 'HBase', 'PyTorch', 'Keras',\n",
    "    'Scikit-learn', 'Pandas', 'NumPy', 'Data Cleaning', 'Data Integration',\n",
    "    'AWS', 'Google Cloud Platform', 'Microsoft Azure', 'MLOps', 'Data Pipeline',\n",
    "    'OLAP', 'OLTP', 'Fog Computing', 'Feature Engineering', 'Dimensionality Reduction',\n",
    "    'Principal Component Analysis', 'Singular Value Decomposition', 'Data Anonymization',\n",
    "    'Privacy Preserving Data Mining', 'GDPR', 'Data Ethics', 'Database Management Systems',\n",
    "    'SQL Server', 'Teradata', 'Data Catalog', 'Data Lineage', 'Data Lakehouse',\n",
    "    'Structured Data', 'Unstructured Data', 'Semi-Structured Data', 'Distributed Computing',\n",
    "    'HDFS', 'Columnar Databases', 'Graph Databases', 'Neo4j', 'Data Preprocessing',\n",
    "    'Data Sampling', 'Data Imputation', 'Anomaly Detection', 'Recurrent Neural Networks',\n",
    "    'Long Short-Term Memory', 'Attention Mechanisms', 'Data Monetization', 'Data Strategy',\n",
    "    'Data Literacy', 'Data Democratization', 'Self-Service Analytics', 'Augmented Analytics',\n",
    "    'Explainable AI', 'AutoML', 'Synthetic Data', 'Data-Driven Decision Making',\n",
    "    'DataOps', 'Data Stewardship', 'Data Scientist', 'Data Engineer', 'Data Analyst',\n",
    "    'Data Architect', 'Chief Data Officer', 'Scalability', 'Throughput', 'Fault Tolerance',\n",
    "    'Load Balancing', 'API', 'RESTful Services', 'Parquet', 'Avro', 'ORC', 'Apache Arrow',\n",
    "    'Data Storage', 'Data Retrieval', 'ETL Tools', 'Informatica', 'Talend', 'Pentaho',\n",
    "    'SSIS', 'Data Enrichment', 'Data Warehouse Automation', 'Data Ingestion', 'Data Retention',\n",
    "    'Data Archiving', 'Data Lifecycle Management', 'Data Replication', 'Master Data Management',\n",
    "    'Reference Data', 'Data Provenance', 'Data Virtualization', 'Data Federation',\n",
    "    'Data Consolidation', 'Data Blending', 'Data Cubes', 'Data Mesh', 'Data Fabric',\n",
    "    'Data Tokenization', 'Data Security', 'Data Privacy', 'Data Loss Prevention',\n",
    "    'Access Control', 'Authentication', 'Authorization', 'Role-Based Access Control',\n",
    "    'Identity and Access Management', 'Encryption at Rest', 'Encryption in Transit',\n",
    "    'SSL', 'TLS', 'Single Sign-On', 'Data Compliance', 'HIPAA', 'PCI DSS', 'Data Breach',\n",
    "    'Data Incident Response', 'High Availability', 'Serverless Computing',\n",
    "    'Function as a Service', 'Platform as a Service', 'Infrastructure as a Service',\n",
    "    'Software as a Service', 'Apache Iceberg', 'Apache Hudi', 'CAP Theorem',\n",
    "    'Consistency', 'Availability', 'Partition Tolerance', 'ACID', 'BASE', 'CQRS',\n",
    "    'Message Queues', 'RabbitMQ', 'ActiveMQ', 'ZeroMQ', 'MQTT', 'Streaming Data',\n",
    "    'Kafka Streams', 'Kinesis', 'Pub/Sub', 'Event Hubs', 'Continuous Integration',\n",
    "    'Continuous Deployment', 'Agile Methodology', 'Scrum', 'Data Transformation',\n",
    "    'Data Normalization', 'Standardization', 'One-Hot Encoding', 'Label Encoding',\n",
    "    'Cross-Validation', 'Random Search', 'K-Fold Cross Validation', 'Ensemble Methods',\n",
    "    'Stacking', 'Transfer Learning', 'Model Deployment', 'Model Serving', 'Model Monitoring',\n",
    "    'A/B Testing', 'Concept Drift', 'Model Retraining', 'Model Explainability',\n",
    "    'SHAP Values', 'LIME', 'Partial Dependence Plots', 'Data Annotation', 'Labeling',\n",
    "    'Overfitting', 'Underfitting', 'Bias-Variance Tradeoff', 'Regularization',\n",
    "    'Elastic Net', 'Dropout', 'Early Stopping', 'Mini-Batch Gradient Descent',\n",
    "    'Adam Optimizer', 'Activation Functions', 'Sigmoid Function', 'Softmax Function',\n",
    "    'Cost Function', 'Loss Function', 'Backpropagation', 'Epoch', 'Batch Size',\n",
    "    'Data Silo', 'Edge AI', 'Data Compression', 'UMAP', 'Data Partitioning',\n",
    "    'Data Sharding', 'Bloom Filters', 'Data Streaming Algorithms', 'Real-Time Analytics',\n",
    "    'Apache Druid', 'ClickHouse', 'OLAP Cubes', 'Data Marketplace', 'Data Exchange',\n",
    "    'Data Brokerage', 'Data Licensing', 'Data Contracts', 'Data Quality Metrics',\n",
    "    'Data Accuracy', 'Data Completeness', 'Data Timeliness', 'Data Conformity',\n",
    "    'Data Uniqueness', 'Data Validation', 'Data Profiling', 'Data Observability',\n",
    "    'Data SLOs', 'Anonymization Techniques', 'K-Anonymity', 'Differential Privacy',\n",
    "    'Homomorphic Encryption', 'Secure Multi-Party Computation', 'Zero-Knowledge Proofs',\n",
    "    'Blockchain', 'Smart Contracts', 'Consensus Algorithms', 'Proof of Work',\n",
    "    'Proof of Stake', 'Machine Learning Lifecycle', 'Data Acquisition', 'Data Processing',\n",
    "    'Model Building', 'Model Evaluation', 'Model Maintenance', 'Experiment Tracking',\n",
    "    'Weights & Biases', 'Data Privacy Laws', 'CCPA', 'Data Residency', 'Hybrid Cloud',\n",
    "    'Multicloud', 'Data Compression Techniques', 'Video Analytics', 'Audio Analytics',\n",
    "    'Topic Modeling', 'Hierarchical Clustering', 'DBSCAN', 'Affinity Propagation',\n",
    "    'Mean Shift', 'K-Nearest Neighbors', 'Multilayer Perceptron', 'Polynomial Regression',\n",
    "    'Ridge Regression', 'Lasso Regression', 'Elastic Net Regression', 'ARIMA Models',\n",
    "    'Autocorrelation', 'Cross-Correlation', 'Spectral Analysis', 'Fourier Transform',\n",
    "    'Wavelets', 'Mutual Information', 'Chi-Square Test', 'Recursive Feature Elimination',\n",
    "    'Univariate Selection', 'Canonical Correlation Analysis', 'Discriminant Analysis',\n",
    "    'Cluster Analysis', 'Cohort Analysis', 'Customer Lifetime Value', 'Churn Prediction',\n",
    "    'Market Basket Analysis', 'Apriori Algorithm', 'FP-Growth Algorithm', 'Hybrid Recommenders',\n",
    "    'Matrix Factorization', 'Isolation Forest', 'One-Class SVM', 'Local Outlier Factor',\n",
    "    'CRISP-DM', 'SEMMA', 'KDD', 'Information Retrieval', 'Named Entity Recognition',\n",
    "    'Latent Dirichlet Allocation', 'Latent Semantic Analysis', 'Word Embeddings', 'GloVe',\n",
    "    'BERT', 'Autoencoders', 'Data Masking', 'Data Swapping', 'Re-identification Risk',\n",
    "    'DAMA-DMBOK', 'DCAM', 'Data Stewardship Council', 'Data Quality Tools',\n",
    "    'Data Integration Tools', 'Data Governance Tools', 'Data Catalog Tools',\n",
    "    'Data Lineage Tools', 'Data Masking Tools', 'Data Encryption Tools',\n",
    "    'Data Backup and Recovery Tools', 'Cloud Storage', 'Object Storage', 'Block Storage',\n",
    "    'File Storage', 'SAN', 'NAS', 'Storage Tiers', 'Cold Storage', 'Warm Storage',\n",
    "    'Data Retention Policies', 'Data Disposal', 'Data Destruction', 'Shredding',\n",
    "    'Degaussing', 'Physical Destruction', 'Data Forensics', 'Incident Response',\n",
    "    'SIEM', 'SOAR', 'Threat Intelligence', 'Vulnerability Management',\n",
    "    'Penetration Testing', 'Ethical Hacking', 'Security Awareness Training', 'Phishing',\n",
    "    'Malware', 'Ransomware', 'Security Monitoring', 'Logging', 'Auditing',\n",
    "    'Compliance Auditing', 'Regulatory Compliance', 'Data Legislation',\n",
    "    'Privacy Impact Assessment', 'Risk Management', 'Business Continuity Planning',\n",
    "    'Green Computing', 'Energy Efficiency', 'Thermal Management', 'Cooling Systems',\n",
    "    'Virtualization', 'Hypervisors', 'Virtual Machines', 'Containers',\n",
    "    'Cloud Native Applications', 'Workflow Management', 'Apache Airflow', 'Luigi',\n",
    "    'Azkaban', 'Data Flow', 'ETL vs ELT', 'Change Data Capture', 'Query Optimization',\n",
    "    'Cost-Based Optimization', 'Rule-Based Optimization', 'Data Formats', 'YAML',\n",
    "    'ProtoBuf', 'Thrift', 'Data Serialization', 'Data Deserialization', 'Marshaling',\n",
    "    'Unmarshaling', 'Operational Data Store', 'Staging Area', 'Sandbox Environment',\n",
    "    'Data Environments', 'Version Control', 'Branching Strategies', 'Trunk-Based Development',\n",
    "    'Code Review', 'Pull Requests', 'Merge Requests', 'Build Automation', 'Test Automation',\n",
    "    'Unit Testing', 'Integration Testing', 'System Testing', 'Acceptance Testing',\n",
    "    'Regression Testing', 'Test-Driven Development', 'Behavior-Driven Development',\n",
    "    'API Contracts', 'SLAs', 'SLOs', 'KPIs', 'Metrics', 'Monitoring', 'Alerting',\n",
    "    'Observability', 'Tracing', 'Metrics Collection', 'Dashboards', 'Reports',\n",
    "    'Choropleth Maps', 'Bar Charts', 'Line Charts', 'Pie Charts', 'Interactive Visualizations',\n",
    "    'Natural Language Query', 'Voice Interfaces', 'Chatbots', 'Virtual Assistants',\n",
    "    'Cognitive Computing', 'Knowledge Graphs', 'Ontologies', 'Taxonomies', 'RDF',\n",
    "    'SPARQL', 'OWL', 'Open Data', 'Data Sharing', 'FAIR Principles', 'Data Repositories',\n",
    "    'Data Archives', 'Data Curation', 'Data Lifecycle', 'Data Management Plan',\n",
    "    'Metadata Standards', 'Dublin Core', 'Data Citation', 'Data Journals', 'Open Science',\n",
    "    'Data Collaborations', 'Data Consortia', 'Data Partnerships', 'Data Commons',\n",
    "    'Data Cooperatives', 'Data Crowdsourcing', 'Citizen Science','algorithm'\n",
    "]\n",
    "\n",
    "lowercase_terms = [term.lower() for term in domain_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d2436-8f8c-45b7-9017-f6fd9d468b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformers import pipeline\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка дообученной модели и токенизатора\n",
    "model_path = \"./trained_scibert_ner_model\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_path, tokenizer=model_path, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Список исключаемых сущностей\n",
    "EXCLUDED_ENTITIES = {\"data\", \"big data\", \"drivers\", \"dedicated\", \"driver\"}\n",
    "\n",
    "# Удаление субтокенов, коротких, повторяющихся и нерелевантных сущностей\n",
    "def postprocess_entities(entities):\n",
    "    \"\"\"\n",
    "    Удаляет некорректные, короткие, повторяющиеся и нерелевантные сущности.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    processed = [\n",
    "        {\"text\": entity[\"text\"].replace(\"##\", \"\").strip().lower(), \"label\": entity[\"label\"]}\n",
    "        for entity in entities\n",
    "    ]\n",
    "    return [\n",
    "        e for e in processed\n",
    "        if len(e[\"text\"]) > 3 and e[\"text\"] not in EXCLUDED_ENTITIES and (e[\"text\"], e[\"label\"]) not in seen and not seen.add((e[\"text\"], e[\"label\"]))\n",
    "    ]\n",
    "\n",
    "# Извлечение сущностей\n",
    "def extract_entities_ner(text):\n",
    "    \"\"\"\n",
    "    Извлекает и обрабатывает сущности из текста.\n",
    "    \"\"\"\n",
    "    results = ner_pipeline(text)\n",
    "    entities = [{\"text\": res[\"word\"], \"label\": res[\"entity_group\"]} for res in results]\n",
    "    return postprocess_entities(entities)\n",
    "\n",
    "# Подсчет статистики\n",
    "def compute_statistics(entities):\n",
    "    \"\"\"\n",
    "    Вычисляет статистику по извлеченным сущностям.\n",
    "    \"\"\"\n",
    "    total_entities = len(entities)\n",
    "    unique_entities = set(entity[\"text\"] for entity in entities)\n",
    "    category_counts = Counter(entity[\"label\"] for entity in entities)\n",
    "    avg_length = sum(len(entity[\"text\"]) for entity in entities) / total_entities if total_entities else 0\n",
    "    min_length = min((len(entity[\"text\"]) for entity in entities), default=0)\n",
    "    max_length = max((len(entity[\"text\"]) for entity in entities), default=0)\n",
    "\n",
    "    return {\n",
    "        \"total\": total_entities,\n",
    "        \"unique\": len(unique_entities),\n",
    "        \"category_counts\": category_counts,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"min_length\": min_length,\n",
    "        \"max_length\": max_length,\n",
    "    }\n",
    "\n",
    "# Вывод статистики\n",
    "def print_statistics(stats):\n",
    "    \"\"\"\n",
    "    Выводит статистику на экран.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Общая статистика ===\")\n",
    "    print(f\"Всего извлечено сущностей: {stats['total']}\")\n",
    "    print(f\"Уникальных сущностей: {stats['unique']}\")\n",
    "    for category, count in stats['category_counts'].items():\n",
    "        print(f\" - {category}: {count} сущностей\")\n",
    "    print(f\"Средняя длина сущности: {stats['avg_length']:.2f} символов\")\n",
    "    print(f\"Минимальная длина: {stats['min_length']} символов\")\n",
    "    print(f\"Максимальная длина: {stats['max_length']} символов\")\n",
    "    print(\"=======================\\n\")\n",
    "\n",
    "# Построение графика распределения категорий\n",
    "def plot_category_distribution(category_counts, title=\"Распределение категорий\"):\n",
    "    \"\"\"\n",
    "    Визуализация распределения категорий.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(category_counts.keys(), category_counts.values(), color='skyblue')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel('Количество сущностей', fontsize=12)\n",
    "    plt.xlabel('Категории', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Анализ документов из MongoDB\n",
    "def process_documents(limit=10):\n",
    "    \"\"\"\n",
    "    Анализирует документы из MongoDB и выводит обработанные сущности со статистикой.\n",
    "    \"\"\"\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    collection = client['VKR1']['proccessed_for_topic_modelling']\n",
    "    cursor = collection.find({}, {\"_id\": 1, \"abstract_bert_emb\": 1}).limit(limit)\n",
    "\n",
    "    all_entities = []\n",
    "\n",
    "    for doc in tqdm.tqdm(cursor, desc=\"NER-анализ\"):\n",
    "        text = doc.get(\"abstract_bert_emb\", \"\")\n",
    "        if not text or len(text) <= 50:\n",
    "            continue\n",
    "\n",
    "        entities = extract_entities_ner(text)\n",
    "        all_entities.extend(entities)\n",
    "        print(f\"Документ ID {doc['_id']}:\")\n",
    "        if entities:\n",
    "            for entity in entities:\n",
    "                print(f\" - Сущность: {entity['text']}, Категория: {entity['label']}\")\n",
    "            stats = compute_statistics(entities)\n",
    "            print_statistics(stats)\n",
    "        else:\n",
    "            print(\" - Сущности не найдены.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Итоговая статистика\n",
    "    print(\"\\n=== Итоговая статистика по всем документам ===\")\n",
    "    total_stats = compute_statistics(all_entities)\n",
    "    print_statistics(total_stats)\n",
    "\n",
    "    # Построение графика\n",
    "    plot_category_distribution(total_stats['category_counts'], title=\"Итоговое распределение категорий\")\n",
    "\n",
    "    # Частотный анализ сущностей\n",
    "    entity_frequencies = Counter(entity[\"text\"] for entity in all_entities)\n",
    "    print(\"Топ 10 самых частых сущностей:\")\n",
    "    for entity, freq in entity_frequencies.most_common(10):\n",
    "        print(f\"{entity}: {freq} раз(а)\")\n",
    "\n",
    "# Запуск анализа документов\n",
    "process_documents(limit=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
